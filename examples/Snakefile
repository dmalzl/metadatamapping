from pathlib import Path

PROJECT_DIR = Path(workflow.basedir).resolve().parents[2]
configfile: PROJECT_DIR / "config.yaml"

rule archs4_accessions_to_srauids:
    """
    Retrieve SRA UIDs for each sample in the ARCHS4 file
    This can take quite some time (ran ~48 - 72h with 4 cores)
    """
    input: 
        PROJECT_DIR / config["archs4_h5"]
    output: 
        PROJECT_DIR / "metadata/archs4_accession_uids.tsv"
    conda:
        PROJECT_DIR / "envs/metadatamapping.yaml"
    threads: config['archs4_to_uid']['n_processes']
    params:
        entrez_email = config['entrez']['email'],
        entrez_api_key = config['entrez']['api_key']

    run:
        from metadatamapping import archs4, metadata
        from Bio import Entrez
        import logging

        Entrez.email = entrez_email
        Entrez.api_key = entrez_api_key

        logger = logging.getLogger()
        logger.setLevel(logging.INFO)

        # this might be nice to have as config but I leave it as is for now
        retain_keys = [
            'geo_accession', 'characteristics_ch1', 'molecule_ch1', 'readsaligned', 'relation', 
            'series_id', 'singlecellprobability', 'source_name_ch1', 'title'
        ]
        archs4_metadata = archs4.get_filtered_sample_metadata(
            input[0],
            retain_keys
        )

        metadata.map_accessions_to_srauids(
            archs4_metadata,
            output[0],
            n_processes = threads
        )


rule gzip_archs4_accession_srauid_map:
    """
    compresses the raw textfile to save space
    """
    input:
        PROJECT_DIR / "metadata/archs4_accession_uids.tsv"
    output:
        PROJECT_DIR / "metadata/archs4_accession_uids.tsv.gz"
    shell:
        "gzip {input}"
    

rule accessions_from_srauids:
    """
    uses the retrieved SRA UIDs and fetches all associated accessions
    from the SRA server using eSummary
    """
    input:
        PROJECT_DIR / "metadata/archs4_accession_uids.tsv.gz"
    output:
        PROJECT_DIR / "metadata/ncbi_accessions_from_srauids.tsv.gz"
    conda:
        PROJECT_DIR / "envs/metadatamapping.yaml"
    params:
        entrez_email = config['entrez']['email'],
        entrez_api_key = config['entrez']['api_key']
    
    run:
        import logging
        
        import pandas as pd

        from metadatamapping import metadata
        from Bio import Entrez

        Entrez.email = entrez_email
        Entrez.api_key = entrez_api_key

        logger = logging.getLogger()
        logger.setLevel(logging.INFO)

        archs4_accession_uids = pd.read_csv(
            input[0],
            sep = '\t',
            compression = 'gzip'
        )

        archs4_accession_uids.drop_duplicates(inplace = True)

        ncbi_accessions = metadata.srauids_to_accessions(
            archs4_accession_uids
        )

        ncbi_accessions.to_csv(
            output[0],
            sep = '\t',
            compression = 'gzip'
        )


rule link_sra_uids_to_biosample_uids:
    """
    links SRA UIDs to BioSample UIDs to subsequently retrieve BioSample metadata
    """
    input:
        PROJECT_DIR / "metadata/ncbi_accessions_from_srauids.tsv.gz"
    output:
        PROJECT_DIR / "metadata/srauids_to_biosampleuids.tsv.gz"
    conda:
        PROJECT_DIR / "envs/metadatamapping.yaml"
    params:
        entrez_email = config['entrez']['email'],
        entrez_api_key = config['entrez']['api_key']
    
    run:
        import logging
        
        import pandas as pd

        from metadatamapping import link
        from Bio import Entrez

        Entrez.email = entrez_email
        Entrez.api_key = entrez_api_key

        logger = logging.getLogger()
        logger.setLevel(logging.INFO)

        ncbi_accessions = pd.read_csv(
            input[0],
            sep = '\t',
            compression = 'gzip'
        )

        srauids_to_biosampleuids = link.link_sra_to_biosample(
            ncbi_accessions.uid
        )

        srauids_to_accessions.dropna(inplace = True)

        ncbi_accessions.to_csv(
            output[0],
            sep = '\t',
            compression = 'gzip'
        )


rule collect_metadata_from_biosample:
    """
    uses the linked biosample UIDs to fetch available metadata from NCBI BioSample
    """
    input:
        PROJECT_DIR / "metadata/srauids_to_biosampleuids.tsv.gz"
    output:
        PROJECT_DIR / "metadata/biosample_metadata.tsv.gz"
    conda:
        PROJECT_DIR / "envs/metadatamapping.yaml"
    params:
        entrez_email = config['entrez']['email'],
        entrez_api_key = config['entrez']['api_key']
    
    run:
        import logging
        
        import pandas as pd

        from metadatamapping import metadata
        from Bio import Entrez

        Entrez.email = entrez_email
        Entrez.api_key = entrez_api_key

        logger = logging.getLogger()
        logger.setLevel(logging.INFO)

        srauids_to_biosampleuids = pd.read_csv(
            input[0],
            sep = '\t',
            compression = 'gzip'
        )

        biosample_metadata = metadata.biosample_uids_to_metadata(
            srauids_to_biosampleuids.biosample
        )

        biosample_metadata.to_csv(
            output[0],
            sep = '\t',
            compression = 'gzip'
        )


# I actually don't know how to make rules conditional
# this should probably be a configurable telling the pipeline to get the 
# normalized metadata from the MetaSRA server or do the mapping locally (see below)
rule collect_metadata_from_metasra_server:
    """
    uses the linked biosample UIDs to fetch available metadata from NCBI BioSample
    """
    input:
        PROJECT_DIR / "metadata/ncbi_accessions_from_srauids.tsv.gz"
    output:
        PROJECT_DIR / "metadata/metasra_metadata.tsv.gz"
    conda:
        PROJECT_DIR / "envs/metadatamapping.yaml"    
    run:
        import logging
        
        import pandas as pp

        from metadatamapping import metadata

        logger = logging.getLogger()
        logger.setLevel(logging.INFO)

        ncbi_accessions = pd.read_csv(
            input[0],
            sep = '\t',
            compression = 'gzip'
        )

        metasra_metadata = metadata.metasra_from_study_id(
            ncbi_accessions.study.unique()
        )

        metasra_metadata.to_csv(
            output[0],
            sep = '\t',
            compression = 'gzip'
        )


# do the mapping locally by splitting the biosample metadata into batches of
# 10k samples and running a modified version of the MetaSRA pipeline on each
# of these batches concurrently to speed up the process (it is very slow)
# however I don't know if this code even runs as I have no experience with
# wildcards and their specifics in snakemake
rule split_raw_metadata_into_batches:
    """
    splits the acquired BioSample metadata into batches of 2,500 samples
    for parallel normalization with MetaSRA
    """
    input:
        PROJECT_DIR / config["archs4_h5"],
        PROJECT_DIR / "metadata/srauids_to_biosampleuids.tsv.gz",
        PROJECT_DIR / "metadata/biosample_metadata.tsv.gz",
        PROJECT_DIR / "metadata/ncbi_accessions_from_srauids.tsv.gz"
    output:
        PROJECT_DIR / "raw/raw_biosample_metadata_{id}.json"
    conda:
        PROJECT_DIR / "envs/metadatamapping.yaml"
    run:
        from metadatamapping import metadata, archs4, metasra

        def read_compessed_tsv(filename):
            df = pd.read_csv(
                filename,
                sep = '\t',
                compression = 'gzip'
            )
            return df

        srauids_to_biosampleuids = read_compessed_tsv(input[1])
        biosample_metadata = read_compessed_tsv(input[2])
        ncbi_accessions = read_compessed_tsv(input[3])

        retain_keys = [
            'geo_accession', 'characteristics_ch1', 'molecule_ch1', 'readsaligned', 'relation', 
            'series_id', 'singlecellprobability', 'source_name_ch1', 'title'
        ]
        archs4_metadata = archs4.get_filtered_sample_metadata(
            input[0],
            retain_keys
        )

        archs4_annotated = metadata.merge_to_annotated_metadata_frame(
            biosample_metadata,
            srauids_to_biosampleuids,
            ncbi_accessions,
            archs4_metadata
        )

        accession_and_attributes = metasra.make_accession_and_attributes_table(
            archs4_annotated
        )
        
        metasra.raw_metadata_to_json(
            accession_and_attributes, 
            output[0], 
            chunksize = 2500
        )


rule normalize_metadata_with_metasra:
    """
    uses a modified version of the MetaSRA pipeline to normalize metadata fetched from BioSample
    """
    input:
        PROJECT_DIR / "raw/raw_biosample_metadata_{id}.json"
    output:
        PROJECT_DIR / "normalized/normalized_biosample_metadata_{id}.json"
    conda:
        PROJECT_DIR / "envs/metasra.yaml"
    shell:
        "scripts/run_metasra.sh {input} {output}"
    

rule collect_metadata_from_metasra_pipeline:
    """
    gathers the outputs of the mapping stage and converts it to a pandas.DataFrame
    """
    input:
        expand(PROJECT_DIR / "raw/raw_biosample_metadata_{id}.json", id = ?) # don't know how to write this for aggregation
    output:
        PROJECT_DIR / "metadata/metasra_metadata.tsv.gz"
    conda:
        PROJECT_DIR / "envs/metadatamapping.yaml"
    run:
        from metadatamapping import metasra, obo
        import pandas as pd

        import json
        import os

        # read ontologies used for normalizing metadata
        metasra_path = config['metasra_path']
        with open(os.path.join(metasra_path, 'map_sra_to_ontology/ont_prefix_to_filename.json'), 'r') as f:
            ontology_files = json.load(f)
            
        for k, v in ontology_files.items():
            ontology_files[k] = os.path.join(metasra_path, 'map_sra_to_ontology/obo/', v)

        ontologies = [
            obo.make_id_to_term_dictionary(obo_file) for obo_file in ontology_files.values()
        ]

        mappings = [
            metasra.metasra_output_json_to_dataframe(jsonfile, ontologies) for jsonfile in input
        ]

        metasra_mappings = pd.concat(mappings)

        metasra_mappings.to_csv(
            output[0],
            sep = '\t',
            compression = 'gzip'
        )
        

rule generate_anndata_file:
    """
    collects the results of all steps, extracts the relevant expression data
    from the ARCHS4 file and generates an h5ad file from it. This may need a lot of memory
    For extracting 720k samples we had to use ~300GB of RAM.
    """
    input:
        PROJECT_DIR / config["archs4_h5"],
        PROJECT_DIR / "metadata/srauids_to_biosampleuids.tsv.gz",
        PROJECT_DIR / "metadata/biosample_metadata.tsv.gz",
        PROJECT_DIR / "metadata/ncbi_accessions_from_srauids.tsv.gz",
        PROJECT_DIR / "metadata/metasra_metadata.tsv.gz"
    output:
        PROJECT_DIR / "data/archs4_metasra.h5ad"
    threads: config['archs4_data_extraction']['n_processes']
    conda:
        PROJECT_DIR / "envs/metadatamapping.yaml"    
    run:
        import anndata as ad
        import pandas as pd
        from metadatamapping import archs4, metadata

        def read_compessed_tsv(filename):
            df = pd.read_csv(
                filename,
                sep = '\t',
                compression = 'gzip'
            )
            return df


        srauids_to_biosampleuids = read_compessed_tsv(input[1])
        biosample_metadata = read_compessed_tsv(input[2])
        ncbi_accessions = read_compessed_tsv(input[3])
        metasra_metadata = read_compessed_tsv(input[4])

        retain_keys = [
            'geo_accession', 'characteristics_ch1', 'molecule_ch1', 'readsaligned', 'relation', 
            'series_id', 'singlecellprobability', 'source_name_ch1', 'title'
        ]
        archs4_metadata = archs4.get_filtered_sample_metadata(
            input[0],
            retain_keys
        )

        archs4_annotated = metadata.merge_to_annotated_metadata_frame(
            biosample_metadata,
            srauids_to_biosampleuids,
            ncbi_accessions,
            archs4_metadata
        )

        archs4_metasra = metadata.annotated_with_metasra_and_treatment(
            archs4_annotated,
            metasra_metadata
        )

        archs4_data = archs4.samples(
            input[0],
            archs4_metasra.set_index('geo'),
            n_processes = threads
        )

        archs4_data.write(output[0])
